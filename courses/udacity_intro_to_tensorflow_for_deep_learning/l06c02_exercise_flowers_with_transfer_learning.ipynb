{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l06c02_exercise_flowers_with_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bbc1d15ea70a42409c633333eee5d471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13be9bf473f449e2aad998e5bd21de9a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3f4675d11a184e6e83a0bb516a238f14",
              "IPY_MODEL_1300a3be613b4846a5d5cc4e603b0adb",
              "IPY_MODEL_f90964d8bd37471aa6306289f092d950"
            ]
          }
        },
        "13be9bf473f449e2aad998e5bd21de9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f4675d11a184e6e83a0bb516a238f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4bdbf5046e2a4ab7875dd5a2da78399e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Dl Completed...: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_725a60ce1f5a44aba9bcefda02dad01f"
          }
        },
        "1300a3be613b4846a5d5cc4e603b0adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5a5f44ecd8a4d9ea4a10cda7cd42db4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7b63d44ac81447eb0da383d422da4a6"
          }
        },
        "f90964d8bd37471aa6306289f092d950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f03846ece5bd4e9392b0805b418c40fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:04&lt;00:00,  1.00s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b86b565df074f56aa01dfb9def3f15e"
          }
        },
        "4bdbf5046e2a4ab7875dd5a2da78399e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "725a60ce1f5a44aba9bcefda02dad01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5a5f44ecd8a4d9ea4a10cda7cd42db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7b63d44ac81447eb0da383d422da4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f03846ece5bd4e9392b0805b418c40fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b86b565df074f56aa01dfb9def3f15e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_tvPdyfA-BL"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0O_LFhwSBCjm"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-3Pry4jh1-E"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l06c02_exercise_flowers_with_transfer_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l06c02_exercise_flowers_with_transfer_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxjpzKTvg_dd"
      },
      "source": [
        "# TensorFlow Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crU-iluJIEzw"
      },
      "source": [
        "[TensorFlow Hub](http://tensorflow.org/hub) is an online repository of already trained TensorFlow models that you can use.\n",
        "These models can either be used as is, or they can be used for Transfer Learning.\n",
        "\n",
        "Transfer learning is a process where you take an existing trained model, and extend it to do additional work. This involves leaving the bulk of the model unchanged, while adding and retraining the final layers, in order to get a different set of possible outputs.\n",
        "\n",
        "Here, you can see all the models available in [TensorFlow Module Hub](https://tfhub.dev/).\n",
        "\n",
        "Before starting this Colab, you should reset the Colab environment by selecting `Runtime -> Reset all runtimes...` from menu above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RVsYZLEpEWs"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUCEcRdhnyWn"
      },
      "source": [
        "Some normal imports we've seen before. The new one is importing tensorflow_hub which this Colab will make heavy use of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIuDCLW_IAG_"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHenfza_ICJL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEsgwsqbHFn2"
      },
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amfzqn1Oo7Om"
      },
      "source": [
        "# TODO: Download the Flowers Dataset using TensorFlow Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z93vvAdGxDMD"
      },
      "source": [
        "In the cell below you will download the Flowers dataset using TensorFlow Datasets. If you look at the [TensorFlow Datasets documentation](https://www.tensorflow.org/datasets/datasets#tf_flowers) you will see that the name of the Flowers dataset is `tf_flowers`. You can also see that this dataset is only split into a TRAINING set. You will therefore have to use `tfds.splits` to split this training set into to a `training_set` and a `validation_set`. Do a `[70, 30]` split such that 70 corresponds to the `training_set` and 30 to the `validation_set`. Then load the `tf_flowers` dataset using `tfds.load`. Make sure the `tfds.load` function uses the all the parameters you need, and also make sure it returns the dataset info, so we can retrieve information about the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXiJjX0jfx1o",
        "outputId": "09e6a0b4-79bc-477b-bc4b-e2302e27a954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "bbc1d15ea70a42409c633333eee5d471",
            "13be9bf473f449e2aad998e5bd21de9a",
            "3f4675d11a184e6e83a0bb516a238f14",
            "1300a3be613b4846a5d5cc4e603b0adb",
            "f90964d8bd37471aa6306289f092d950",
            "4bdbf5046e2a4ab7875dd5a2da78399e",
            "725a60ce1f5a44aba9bcefda02dad01f",
            "a5a5f44ecd8a4d9ea4a10cda7cd42db4",
            "a7b63d44ac81447eb0da383d422da4a6",
            "f03846ece5bd4e9392b0805b418c40fe",
            "7b86b565df074f56aa01dfb9def3f15e"
          ]
        }
      },
      "source": [
        "\n",
        "\n",
        "(training_set, validation_set), dataset_info = tfds.load(name='tf_flowers',\n",
        "                                                         split = ['train[:70%]', 'train[70%:]'],\n",
        "                                                         as_supervised=True,\n",
        "                                                         with_info=True\n",
        "\n",
        "                                                         )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset tf_flowers/3.0.1 (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.1...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbc1d15ea70a42409c633333eee5d471",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mDataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0p1sOEHf0JF"
      },
      "source": [
        "# TODO: Print Information about the Flowers Dataset\n",
        "\n",
        "Now that you have downloaded the dataset, use the dataset info to print the number of classes in the dataset, and also write some code that counts how many images we have in the training and validation sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrIUV3V0xDL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b184ba-5f49-45ba-c772-dca6db0ea652"
      },
      "source": [
        "num_training_examples = len(training_set)\n",
        "num_validation_examples = len(validation_set)\n",
        "num_images = num_training_examples + num_validation_examples\n",
        "num_classes = dataset_info.features['label'].num_classes\n",
        "\n",
        "\n",
        "print('Total Number of Classes: {}'.format(num_classes))\n",
        "print('Total Number of Training Images: {}'.format(num_training_examples))\n",
        "print('Total Number of Validation Images: {} \\n'.format(num_validation_examples))\n",
        "print('Total Number of Images: {} \\n'.format(num_images))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Classes: 5\n",
            "Total Number of Training Images: 2569\n",
            "Total Number of Validation Images: 1101 \n",
            "\n",
            "Total Number of Images: 3670 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlFZ_hwjCLgS"
      },
      "source": [
        "The images in the Flowers dataset are not all the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4lDPkn2cpWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261a622b-69ef-4d53-da12-f0fe5fc08771"
      },
      "source": [
        "for i, example in enumerate(training_set.take(5)):\n",
        "  print('Image {} shape: {} label: {}'.format(i+1, example[0].shape, example[1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1 shape: (333, 500, 3) label: 2\n",
            "Image 2 shape: (212, 320, 3) label: 3\n",
            "Image 3 shape: (240, 320, 3) label: 3\n",
            "Image 4 shape: (240, 320, 3) label: 4\n",
            "Image 5 shape: (317, 500, 3) label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbgpD3E6gM2P"
      },
      "source": [
        "# TODO: Reformat Images and Create Batches\n",
        "\n",
        "In the cell below create a function that reformats all images to the resolution expected by MobileNet v2 (224, 224) and normalizes them. The function should take in an `image` and a `label` as arguments and should return the new `image` and corresponding `label`. Then create training and validation batches of size `32`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we_ftzQxNf7e"
      },
      "source": [
        "IMAGE_RES = 224\n",
        "\n",
        "def format_image(image, label):\n",
        "  \n",
        "  image = tf.image.resize(image,size=(IMAGE_RES, IMAGE_RES)) / 255.0\n",
        "  return image, label\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_batches = training_set.shuffle(buffer_size=len(training_set)).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzV457OXreQP"
      },
      "source": [
        "# Do Simple Transfer Learning with TensorFlow Hub\n",
        "\n",
        "Let's now use TensorFlow Hub to do Transfer Learning. Remember, in transfer learning we reuse parts of an already trained model and change the final layer, or several layers, of the model, and then retrain those layers on our own dataset.\n",
        "\n",
        "### TODO: Create a Feature Extractor\n",
        "In the cell below create a `feature_extractor` using MobileNet v2. Remember that the partial model from TensorFlow Hub (without the final classification layer) is called a feature vector. Go to the [TensorFlow Hub documentation](https://tfhub.dev/s?module-type=image-feature-vector&q=tf2) to see a list of available feature vectors. Click on the `tf2-preview/mobilenet_v2/feature_vector`. Read the documentation and get the corresponding `URL` to get the MobileNet v2 feature vector. Finally, create a `feature_extractor` by using `hub.KerasLayer` with the correct `input_shape` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wB030nezBwI"
      },
      "source": [
        "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "feature_extractor = hub.KerasLayer(URL,\n",
        "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtFmF7A5E4tk"
      },
      "source": [
        "### TODO: Freeze the Pre-Trained Model\n",
        "\n",
        "In the cell below freeze the variables in the feature extractor layer, so that the training only modifies the final classifier layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egpnj7hm0YSv"
      },
      "source": [
        "feature_extractor.trainable = False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPVeouTksO9q"
      },
      "source": [
        "### TODO: Attach a classification head\n",
        "\n",
        "In the cell below create a `tf.keras.Sequential` model, and add the pre-trained model and the new classification layer. Remember that the classification layer must have the same number of classes as our Flowers dataset. Finally print a summary of the Sequential model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGcY27fY1q3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972e569c-44da-4a4a-c18f-b7d845ad0b9c"
      },
      "source": [
        "model = tf.keras.Sequential(layers=[\n",
        "                                    feature_extractor,\n",
        "                                    layers.Dense(units=num_classes)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 6405      \n",
            "=================================================================\n",
            "Total params: 2,264,389\n",
            "Trainable params: 6,405\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHbXQqIquFxQ"
      },
      "source": [
        "### TODO: Train the model\n",
        "\n",
        "In the cell bellow train this model like any other, by first calling `compile` and then followed by `fit`. Make sure you use the proper parameters when applying both methods. Train the model for only 6 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n0Wb9ylKd8R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "70ee131b-9cfd-4b6a-a5d1-9e4e555d76e9"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "EPOCHS = 6\n",
        "\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data= validation_batches\n",
        "                     )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "11/81 [===>..........................] - ETA: 5s - loss: 0.1644 - accuracy: 0.9545"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6e767a13f7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m history = model.fit(train_batches,\n\u001b[1;32m      9\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalidation_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                      )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76as-K8-vFQJ"
      },
      "source": [
        "You can see we get ~88% validation accuracy with only 6 epochs of training, which is absolutely awesome. This is a huge improvement over the model we created in the previous lesson, where we were able to get ~76% accuracy with 80 epochs of training. The reason for this difference is that MobileNet v2 was carefully designed over a long time by experts, then trained on a massive dataset (ImageNet)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLxTcprUqJaq"
      },
      "source": [
        "# TODO: Plot Training and Validation Graphs\n",
        "\n",
        "In the cell below, plot the training and validation accuracy/loss graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d28dhbFpr98b",
        "outputId": "0150ac86-9ac3-490f-dd6b-0965e2624379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "epoch_range = range(EPOCHS)\n",
        "acc = plt.plot(epoch_range, history.history['accuracy'])\n",
        "\n",
        "val_acc = \n",
        "\n",
        "loss = \n",
        "val_loss = \n",
        "\n",
        "epochs_range = "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmoDisGvNye"
      },
      "source": [
        "What is a bit curious here is that validation performance is better than training performance, right from the start to the end of execution.\n",
        "\n",
        "One reason for this is that validation performance is measured at the end of the epoch, but training performance is the average values across the epoch.\n",
        "\n",
        "The bigger reason though is that we're reusing a large part of MobileNet which is already trained on Flower images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb__ZN8uFn-D"
      },
      "source": [
        "# TODO: Check Predictions\n",
        "\n",
        "In the cell below get the label names from the dataset info and convert them into a NumPy array. Print the array to make sure you have the correct label names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Zvg2i0fzJu"
      },
      "source": [
        "class_names = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Olg6MsNGJTL"
      },
      "source": [
        "### TODO: Create an Image Batch and Make Predictions\n",
        "\n",
        "In the cell below, use the `next()` function to create an `image_batch` and its corresponding `label_batch`. Convert both the `image_batch` and `label_batch` to numpy arrays using the `.numpy()` method. Then use the `.predict()` method to run the image batch through your model and make predictions. Then use the `np.argmax()` function to get the indices of the best prediction for each image. Finally convert the indices of the best predictions to class names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCLVCpEjJ_VP"
      },
      "source": [
        "image_batch, label_batch = \n",
        "\n",
        "\n",
        "\n",
        "predicted_batch = \n",
        "predicted_batch = tf.squeeze(predicted_batch).numpy()\n",
        "\n",
        "predicted_ids = \n",
        "predicted_class_names = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGbZxl9GZs-"
      },
      "source": [
        "### TODO: Print True Labels and Predicted Indices\n",
        "\n",
        "In the cell below, print the true labels and the indices of predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9IhOmGI5dJ"
      },
      "source": [
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJDyzEfYuFcW"
      },
      "source": [
        "# Plot Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC_AYRJU9NQe"
      },
      "source": [
        "plt.figure(figsize=(10,9))\n",
        "for n in range(30):\n",
        "  plt.subplot(6,5,n+1)\n",
        "  plt.subplots_adjust(hspace = 0.3)\n",
        "  plt.imshow(image_batch[n])\n",
        "  color = \"blue\" if predicted_ids[n] == label_batch[n] else \"red\"\n",
        "  plt.title(predicted_class_names[n].title(), color=color)\n",
        "  plt.axis('off')\n",
        "_ = plt.suptitle(\"Model predictions (blue: correct, red: incorrect)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBKxS5CuKhc"
      },
      "source": [
        "# TODO: Perform Transfer Learning with the Inception Model\n",
        "\n",
        "Go to the [TensorFlow Hub documentation](https://tfhub.dev/s?module-type=image-feature-vector&q=tf2) and click on `tf2-preview/inception_v3/feature_vector`. This feature vector corresponds to the Inception v3 model. In the cells below, use transfer learning to create a CNN that uses Inception v3 as the pretrained model to classify the images from the Flowers dataset. Note that Inception, takes as input, images that are 299 x 299 pixels. Compare the accuracy you get with Inception v3 to the accuracy you got with MobileNet v2."
      ]
    }
  ]
}