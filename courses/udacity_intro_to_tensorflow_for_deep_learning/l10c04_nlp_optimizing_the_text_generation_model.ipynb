{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c04_nlp_optimizing_the_text_generation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Optimizing the Text Generation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCxhW3mtLmfb"
      },
      "source": [
        "You've already done some amazing work with generating new songs, but so far we've seen some issues with repetition and a fair amount of incoherence. By using more data and further tweaking the model, you'll be able to get improved results. We'll once again use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics) here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "74a84a23-0c05-4f9f-fa2f-30927f541c96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-26 21:55:35--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.128.139, 142.250.128.100, 142.250.128.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.128.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bk1jkm29jdp88r67lfmlhaueohi4m53l/1635285300000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-10-26 21:55:36--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bk1jkm29jdp88r67lfmlhaueohi4m53l/1635285300000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 142.250.136.132, 2607:f8b0:4001:c34::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|142.250.136.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   174MB/s    in 0.4s    \n",
            "\n",
            "2021-10-26 21:55:37 (174 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz9x-7dWihxx"
      },
      "source": [
        "## 250 Songs\n",
        "\n",
        "Now we've seen a model trained on just a small sample of songs, and how this often leads to repetition as you get further along in trying to generate new text. Let's switch to using the 250 songs instead, and see if our output improves. This will actually be nearly 10K lines of lyrics, which should be sufficient.\n",
        "\n",
        "Note that we won't use the full dataset here as it will take up quite a bit of RAM and processing time, but you're welcome to try doing so on your own later. If interested, you'll likely want to use only some of the more common words for the Tokenizer, which will help shrink processing time and memory needed (or else you'd have an output array hundreds of thousands of words long)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWbMN_19jfRT"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRmPPJegovBe"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIGedF3XjHj4",
        "outputId": "4dae33c0-04dc-45db-e698-90168c5bbe70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "# Read the dataset from csv - this time with 250 songs\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:250]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus, num_words=2000)\n",
        "total_words = tokenizer.num_words\n",
        "\n",
        "# There should be a lot more words now\n",
        "print(total_words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quoDmw_FkNBA"
      },
      "source": [
        "### Create Sequences and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkLAf3HmkPSo"
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECbqT-blMk-"
      },
      "source": [
        "### Train a (Better) Text Generation Model\n",
        "\n",
        "With more data, we'll cut off after 100 epochs to avoid keeping you here all day. You'll also want to change your runtime type to GPU if you haven't already (you'll need to re-run the above cells if you change runtimes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nHOp6uWlP_P",
        "outputId": "40712297-a12c-41a5-ea38-a27913d39820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=100, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1480/1480 [==============================] - 27s 12ms/step - loss: 5.9800 - accuracy: 0.0459\n",
            "Epoch 2/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 5.6575 - accuracy: 0.0523\n",
            "Epoch 3/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 5.4537 - accuracy: 0.0727\n",
            "Epoch 4/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 5.2333 - accuracy: 0.0993\n",
            "Epoch 5/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 5.0405 - accuracy: 0.1185\n",
            "Epoch 6/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 4.8733 - accuracy: 0.1381\n",
            "Epoch 7/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 4.7313 - accuracy: 0.1531\n",
            "Epoch 8/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 4.5975 - accuracy: 0.1678\n",
            "Epoch 9/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 4.4683 - accuracy: 0.1831\n",
            "Epoch 10/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 4.3510 - accuracy: 0.1964\n",
            "Epoch 11/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 4.2466 - accuracy: 0.2083\n",
            "Epoch 12/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 4.1520 - accuracy: 0.2205\n",
            "Epoch 13/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 4.0689 - accuracy: 0.2292\n",
            "Epoch 14/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.9892 - accuracy: 0.2398\n",
            "Epoch 15/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.9142 - accuracy: 0.2492\n",
            "Epoch 16/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.8488 - accuracy: 0.2582\n",
            "Epoch 17/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 3.7867 - accuracy: 0.2663\n",
            "Epoch 18/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 3.7246 - accuracy: 0.2745\n",
            "Epoch 19/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.6771 - accuracy: 0.2812\n",
            "Epoch 20/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.6258 - accuracy: 0.2881\n",
            "Epoch 21/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.5769 - accuracy: 0.2946\n",
            "Epoch 22/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.5264 - accuracy: 0.3020\n",
            "Epoch 23/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.4831 - accuracy: 0.3084\n",
            "Epoch 24/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.4425 - accuracy: 0.3157\n",
            "Epoch 25/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.4003 - accuracy: 0.3220\n",
            "Epoch 26/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.3663 - accuracy: 0.3270\n",
            "Epoch 27/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.3276 - accuracy: 0.3333\n",
            "Epoch 28/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.2948 - accuracy: 0.3389\n",
            "Epoch 29/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.2573 - accuracy: 0.3453\n",
            "Epoch 30/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.2299 - accuracy: 0.3496\n",
            "Epoch 31/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.1965 - accuracy: 0.3552\n",
            "Epoch 32/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.1699 - accuracy: 0.3588\n",
            "Epoch 33/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.1405 - accuracy: 0.3636\n",
            "Epoch 34/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.1168 - accuracy: 0.3661\n",
            "Epoch 35/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.0884 - accuracy: 0.3721\n",
            "Epoch 36/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.0716 - accuracy: 0.3734\n",
            "Epoch 37/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.0377 - accuracy: 0.3788\n",
            "Epoch 38/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.0171 - accuracy: 0.3832\n",
            "Epoch 39/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.0025 - accuracy: 0.3838\n",
            "Epoch 40/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.9746 - accuracy: 0.3894\n",
            "Epoch 41/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.9586 - accuracy: 0.3915\n",
            "Epoch 42/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.9338 - accuracy: 0.3950\n",
            "Epoch 43/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.9136 - accuracy: 0.3985\n",
            "Epoch 44/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.9006 - accuracy: 0.3996\n",
            "Epoch 45/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.8753 - accuracy: 0.4043\n",
            "Epoch 46/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.8638 - accuracy: 0.4050\n",
            "Epoch 47/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.8394 - accuracy: 0.4106\n",
            "Epoch 48/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.8260 - accuracy: 0.4139\n",
            "Epoch 49/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.8028 - accuracy: 0.4163\n",
            "Epoch 50/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7950 - accuracy: 0.4173\n",
            "Epoch 51/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7763 - accuracy: 0.4196\n",
            "Epoch 52/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7694 - accuracy: 0.4217\n",
            "Epoch 53/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7485 - accuracy: 0.4258\n",
            "Epoch 54/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7366 - accuracy: 0.4297\n",
            "Epoch 55/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7171 - accuracy: 0.4312\n",
            "Epoch 56/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7104 - accuracy: 0.4328\n",
            "Epoch 57/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7005 - accuracy: 0.4355\n",
            "Epoch 58/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.6732 - accuracy: 0.4397\n",
            "Epoch 59/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.6568 - accuracy: 0.4425\n",
            "Epoch 60/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.6503 - accuracy: 0.4428\n",
            "Epoch 61/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.6390 - accuracy: 0.4448\n",
            "Epoch 62/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.6259 - accuracy: 0.4466\n",
            "Epoch 63/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.6203 - accuracy: 0.4480\n",
            "Epoch 64/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.6130 - accuracy: 0.4501\n",
            "Epoch 65/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5960 - accuracy: 0.4536\n",
            "Epoch 66/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5819 - accuracy: 0.4541\n",
            "Epoch 67/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5702 - accuracy: 0.4580\n",
            "Epoch 68/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5650 - accuracy: 0.4585\n",
            "Epoch 69/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5538 - accuracy: 0.4610\n",
            "Epoch 70/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5408 - accuracy: 0.4622\n",
            "Epoch 71/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5366 - accuracy: 0.4633\n",
            "Epoch 72/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5303 - accuracy: 0.4645\n",
            "Epoch 73/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5167 - accuracy: 0.4674\n",
            "Epoch 74/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5033 - accuracy: 0.4708\n",
            "Epoch 75/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4994 - accuracy: 0.4702\n",
            "Epoch 76/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4903 - accuracy: 0.4712\n",
            "Epoch 77/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4754 - accuracy: 0.4742\n",
            "Epoch 78/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4670 - accuracy: 0.4761\n",
            "Epoch 79/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4650 - accuracy: 0.4756\n",
            "Epoch 80/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4542 - accuracy: 0.4791\n",
            "Epoch 81/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4435 - accuracy: 0.4794\n",
            "Epoch 82/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4398 - accuracy: 0.4802\n",
            "Epoch 83/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4447 - accuracy: 0.4802\n",
            "Epoch 84/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4201 - accuracy: 0.4841\n",
            "Epoch 85/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4057 - accuracy: 0.4871\n",
            "Epoch 86/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3959 - accuracy: 0.4878\n",
            "Epoch 87/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3987 - accuracy: 0.4878\n",
            "Epoch 88/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3833 - accuracy: 0.4905\n",
            "Epoch 89/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.4154 - accuracy: 0.4841\n",
            "Epoch 90/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3953 - accuracy: 0.4873\n",
            "Epoch 91/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3692 - accuracy: 0.4935\n",
            "Epoch 92/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3547 - accuracy: 0.4969\n",
            "Epoch 93/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3533 - accuracy: 0.4953\n",
            "Epoch 94/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3702 - accuracy: 0.4918\n",
            "Epoch 95/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3611 - accuracy: 0.4935\n",
            "Epoch 96/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3312 - accuracy: 0.4993\n",
            "Epoch 97/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3278 - accuracy: 0.5004\n",
            "Epoch 98/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3199 - accuracy: 0.5013\n",
            "Epoch 99/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3091 - accuracy: 0.5028\n",
            "Epoch 100/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3071 - accuracy: 0.5052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgvIz20nlQcq"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOqmmarvlSLh",
        "outputId": "3d1f6c57-e3c5-4c2b-afd3-a5e63534c9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISLZZGlQlSxh"
      },
      "source": [
        "### Generate better lyrics!\n",
        "\n",
        "This time around, we should be able to get a more interesting output with less repetition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P96oVMk3lU7y",
        "outputId": "e1659707-356d-4e68-ad12-64f66e884e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills me tonight is just a star and all bay and tails you son of me anywhere chance chance help me all day for daddy kiss stone cool write jet outside queen heal sight new mind laughter of rain short your bar in time youre sorry matter tearing fail die cried matter matter for trip fiddle twist rikky til fire stood treasure whom whom heal gain heal wants beggin you mean to still honey is me bumble laah me get it bumble queen dark and for me apart in going away a in going in his girl in the attic lights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgJKV8_oRU9"
      },
      "source": [
        "### Varying the Possible Outputs\n",
        "\n",
        "In running the above, you may notice that the same seed text will generate similar outputs. This is because the code is currently always choosing the top predicted class as the next word. What if you wanted more variance in the output? \n",
        "\n",
        "Switching from `model.predict_classes` to `model.predict_proba` will get us all of the class probabilities. We can combine this with `np.random.choice` to select a given predicted output based on a probability, thereby giving a bit more randomness to our outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZe9gaJeoGVP",
        "outputId": "3cceb206-189f-40c6-9aa5-f611b233184c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test the method with just the first word after the seed text\n",
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "predicted_probs = model.predict(token_list)[0]\n",
        "predicted = np.random.choice([x for x in range(len(predicted_probs))], \n",
        "                             p=predicted_probs)\n",
        "# Running this cell multiple times should get you some variance in output\n",
        "print(predicted)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee7WKgRGrJy1",
        "outputId": "6c96a2ab-9cef-4161-9e30-3c170dea82f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Use this process for the full output generation\n",
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted_probs = model.predict(token_list)[0]\n",
        "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\n",
        "                               p=predicted_probs)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills me just lights go down lights all sure i really need kong will let me get and not me daylight on the same again but i i talk she sees but its time things more me thrill me playground huh touch but you rest gain fireplace easy town excite houses is ours take begin to my heart ive done cause the bluest of fire sweet devotions that all life around smokin fantasy but you gotta leave your dreams times im one another this fancy dont kissin youll show give that there are gimme more kisses saw cried gain mental beach\n"
          ]
        }
      ]
    }
  ]
}