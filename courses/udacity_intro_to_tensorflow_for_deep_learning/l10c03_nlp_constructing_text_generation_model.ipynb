{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "4853a0be-7425-4cfa-b35f-dc08f89aef86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-26 21:50:51--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.111.102, 108.177.111.113, 108.177.111.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.111.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tv3b4uh8s050tnt9lvpk2u4p9bk0meoj/1635285000000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-10-26 21:50:52--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tv3b4uh8s050tnt9lvpk2u4p9bk0meoj/1635285000000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 209.85.200.132, 2607:f8b0:4001:c16::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|209.85.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   152MB/s    in 0.5s    \n",
            "\n",
            "2021-10-26 21:50:53 (152 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "030923e2-3a5b-4cd9-c1aa-6ec924bdfdf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "193367ea-64ae-4f17-a60f-7d1158dadafc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "a6a890a0-317d-4845-a84e-681e7498ff52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 9s 12ms/step - loss: 6.0077 - accuracy: 0.0318\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.4390 - accuracy: 0.0378\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.3669 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.3115 - accuracy: 0.0429\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.2430 - accuracy: 0.0494\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.1686 - accuracy: 0.0499\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.0873 - accuracy: 0.0565\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.9947 - accuracy: 0.0565\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.9019 - accuracy: 0.0757\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.8044 - accuracy: 0.0853\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.7092 - accuracy: 0.0878\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.6112 - accuracy: 0.1029\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.5201 - accuracy: 0.1176\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.4264 - accuracy: 0.1196\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.3302 - accuracy: 0.1352\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.2327 - accuracy: 0.1498\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.1413 - accuracy: 0.1690\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.0473 - accuracy: 0.1746\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.9654 - accuracy: 0.2003\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.8769 - accuracy: 0.2200\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.8008 - accuracy: 0.2356\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.7143 - accuracy: 0.2477\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.6339 - accuracy: 0.2583\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.5736 - accuracy: 0.2841\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.5068 - accuracy: 0.2856\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.4353 - accuracy: 0.3037\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.3687 - accuracy: 0.3153\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.3192 - accuracy: 0.3148\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.2777 - accuracy: 0.3219\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.1864 - accuracy: 0.3305\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.1102 - accuracy: 0.3517\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.0517 - accuracy: 0.3618\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.0007 - accuracy: 0.3708\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.9503 - accuracy: 0.3819\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.8979 - accuracy: 0.3996\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.8321 - accuracy: 0.4067\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.7704 - accuracy: 0.4319\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.7207 - accuracy: 0.4390\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.6733 - accuracy: 0.4420\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.6329 - accuracy: 0.4541\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.5828 - accuracy: 0.4753\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.5318 - accuracy: 0.4788\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.4887 - accuracy: 0.4884\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.4534 - accuracy: 0.4980\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3982 - accuracy: 0.5121\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3578 - accuracy: 0.5227\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3041 - accuracy: 0.5378\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.2649 - accuracy: 0.5404\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.2322 - accuracy: 0.5414\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.2010 - accuracy: 0.5525\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.1533 - accuracy: 0.5575\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1246 - accuracy: 0.5605\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0782 - accuracy: 0.5716\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0391 - accuracy: 0.5843\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0062 - accuracy: 0.5908\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.9712 - accuracy: 0.5994\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.9348 - accuracy: 0.5984\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8875 - accuracy: 0.6171\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8651 - accuracy: 0.6241\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8283 - accuracy: 0.6287\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8057 - accuracy: 0.6357\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7529 - accuracy: 0.6468\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7291 - accuracy: 0.6604\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6958 - accuracy: 0.6559\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6775 - accuracy: 0.6569\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6409 - accuracy: 0.6690\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6123 - accuracy: 0.6786\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6147 - accuracy: 0.6630\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6237 - accuracy: 0.6645\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5725 - accuracy: 0.6766\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5239 - accuracy: 0.6882\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4886 - accuracy: 0.7038\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4624 - accuracy: 0.7028\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4264 - accuracy: 0.7124\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4063 - accuracy: 0.7200\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3861 - accuracy: 0.7220\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3897 - accuracy: 0.7175\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3518 - accuracy: 0.7215\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3208 - accuracy: 0.7366\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2887 - accuracy: 0.7437\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2705 - accuracy: 0.7412\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2643 - accuracy: 0.7462\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2288 - accuracy: 0.7558\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2126 - accuracy: 0.7598\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1967 - accuracy: 0.7649\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1772 - accuracy: 0.7644\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1546 - accuracy: 0.7704\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1495 - accuracy: 0.7709\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2397 - accuracy: 0.7407\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1479 - accuracy: 0.7664\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1057 - accuracy: 0.7805\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0735 - accuracy: 0.7861\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0515 - accuracy: 0.7947\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0307 - accuracy: 0.7926\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0256 - accuracy: 0.8027\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0102 - accuracy: 0.8022\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9938 - accuracy: 0.8037\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9775 - accuracy: 0.8052\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9607 - accuracy: 0.8108\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9434 - accuracy: 0.8118\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9323 - accuracy: 0.8153\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9258 - accuracy: 0.8174\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9175 - accuracy: 0.8088\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9123 - accuracy: 0.8199\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8889 - accuracy: 0.8209\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8802 - accuracy: 0.8209\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8734 - accuracy: 0.8174\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8720 - accuracy: 0.8169\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8519 - accuracy: 0.8148\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8428 - accuracy: 0.8249\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8326 - accuracy: 0.8224\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8377 - accuracy: 0.8169\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8018 - accuracy: 0.8290\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8021 - accuracy: 0.8320\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7840 - accuracy: 0.8340\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7663 - accuracy: 0.8355\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7618 - accuracy: 0.8345\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7580 - accuracy: 0.8345\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7526 - accuracy: 0.8411\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7457 - accuracy: 0.8325\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7298 - accuracy: 0.8451\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7147 - accuracy: 0.8456\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7109 - accuracy: 0.8491\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6942 - accuracy: 0.8471\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6779 - accuracy: 0.8522\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6749 - accuracy: 0.8522\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6615 - accuracy: 0.8542\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6511 - accuracy: 0.8552\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6459 - accuracy: 0.8628\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6654 - accuracy: 0.8502\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6939 - accuracy: 0.8385\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6829 - accuracy: 0.8416\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6540 - accuracy: 0.8562\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6302 - accuracy: 0.8592\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6214 - accuracy: 0.8602\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6086 - accuracy: 0.8643\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5994 - accuracy: 0.8663\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5896 - accuracy: 0.8683\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5845 - accuracy: 0.8633\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5903 - accuracy: 0.8653\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5822 - accuracy: 0.8653\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5780 - accuracy: 0.8663\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5596 - accuracy: 0.8708\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5547 - accuracy: 0.8708\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5527 - accuracy: 0.8683\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5603 - accuracy: 0.8693\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5632 - accuracy: 0.8653\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5826 - accuracy: 0.8582\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5956 - accuracy: 0.8577\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5821 - accuracy: 0.8607\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5544 - accuracy: 0.8658\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5457 - accuracy: 0.8678\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5369 - accuracy: 0.8708\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5286 - accuracy: 0.8673\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5158 - accuracy: 0.8764\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4992 - accuracy: 0.8754\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4891 - accuracy: 0.8804\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4854 - accuracy: 0.8789\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4809 - accuracy: 0.8799\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5569 - accuracy: 0.8572\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4932 - accuracy: 0.8769\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4782 - accuracy: 0.8799\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4690 - accuracy: 0.8829\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4635 - accuracy: 0.8799\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4539 - accuracy: 0.8840\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4504 - accuracy: 0.8885\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4789 - accuracy: 0.8759\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4692 - accuracy: 0.8769\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4751 - accuracy: 0.8774\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4595 - accuracy: 0.8835\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4437 - accuracy: 0.8835\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4321 - accuracy: 0.8905\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4288 - accuracy: 0.8885\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4249 - accuracy: 0.8920\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4219 - accuracy: 0.8880\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4185 - accuracy: 0.8940\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4123 - accuracy: 0.8935\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4106 - accuracy: 0.8920\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4049 - accuracy: 0.8895\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4010 - accuracy: 0.8940\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.8905\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3985 - accuracy: 0.8925\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3886 - accuracy: 0.8981\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3881 - accuracy: 0.8910\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3838 - accuracy: 0.8956\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3811 - accuracy: 0.8930\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3906 - accuracy: 0.8946\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3903 - accuracy: 0.8930\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3870 - accuracy: 0.8910\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3792 - accuracy: 0.9011\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3725 - accuracy: 0.8976\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3728 - accuracy: 0.8951\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3692 - accuracy: 0.8966\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3670 - accuracy: 0.8961\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3630 - accuracy: 0.9021\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3632 - accuracy: 0.8986\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3624 - accuracy: 0.9011\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3593 - accuracy: 0.9016\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.3703 - accuracy: 0.8991\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3746 - accuracy: 0.8915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "f5f2a2b5-d95f-464c-bfbf-862d62005b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "2067cfee-a70a-4e34-a313-14bf1afea5a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills it my ground me standing here cause cause i found out that cause i cry on on give yourself a break break always youll from break am had am had had always had used do do think had do do think had do do think did had do do think break do think had do break had do do think had do do think had do do think had do do think had do do think had do break do had do break do life song had break do were do do were do do were do do were do\n"
          ]
        }
      ]
    }
  ]
}